---
title: Latest 15 Papers - May 28, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Embodied AI
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models](http://arxiv.org/abs/2505.21500v1)** | 2025-05-27 | <details><summary>Proje...</summary><p>Project: https://zju-real.github.io/ViewSpatial-Page/</p></details> |
| **[Visuospatial Cognitive Assistant](http://arxiv.org/abs/2505.12312v2)** | 2025-05-27 | <details><summary>31 pa...</summary><p>31 pages, 10 figures, 6 tables. The implementation and fine-tuned model (ViCA-7B), along with detailed documentation, are publicly available at https://huggingface.co/nkkbr/ViCA. This is a draft technical report. At Professor Hidetoshi Shimodaira's request, his name has been removed from the author list</p></details> |
| **[Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain](http://arxiv.org/abs/2505.18361v2)** | 2025-05-27 | <details><summary>9 pag...</summary><p>9 pages, 8 figures, 5 tables</p></details> |
| **[Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review](http://arxiv.org/abs/2505.20503v1)** | 2025-05-26 |  |
| **[Agentic 3D Scene Generation with Spatially Contextualized VLMs](http://arxiv.org/abs/2505.20129v1)** | 2025-05-26 |  |
| **[Exploring 3D Activity Reasoning and Planning: From Implicit Human Intentions to Route-Aware Planning](http://arxiv.org/abs/2503.12974v2)** | 2025-05-26 |  |
| **[Multimodal 3D Reasoning Segmentation with Complex Scenes](http://arxiv.org/abs/2411.13927v3)** | 2025-05-26 |  |
| **[What Can RL Bring to VLA Generalization? An Empirical Study](http://arxiv.org/abs/2505.19789v1)** | 2025-05-26 |  |
| **[LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study](http://arxiv.org/abs/2505.19510v1)** | 2025-05-26 | ACL 2025 |
| **[DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation](http://arxiv.org/abs/2505.18078v1)** | 2025-05-23 | <details><summary>Our v...</summary><p>Our video demos and code are available at https://DanceTog.github.io/</p></details> |
| **[STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World Understanding?](http://arxiv.org/abs/2503.23765v4)** | 2025-05-22 |  |
| **[Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning](http://arxiv.org/abs/2505.16928v1)** | 2025-05-22 |  |
| **[Perceptual Quality Assessment for Embodied AI](http://arxiv.org/abs/2505.16815v1)** | 2025-05-22 |  |
| **[Praxis-VLM: Vision-Grounded Decision Making via Text-Driven Reinforcement Learning](http://arxiv.org/abs/2503.16965v2)** | 2025-05-22 |  |
| **[DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving](http://arxiv.org/abs/2505.16278v1)** | 2025-05-22 | <details><summary>Proje...</summary><p>Project Page: https://thinklab-sjtu.github.io/DriveMoE/</p></details> |

## Reinforcement Learning
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Reinforcing General Reasoning without Verifiers](http://arxiv.org/abs/2505.21493v1)** | 2025-05-27 |  |
| **[Policy Optimized Text-to-Image Pipeline Design](http://arxiv.org/abs/2505.21478v1)** | 2025-05-27 |  |
| **[Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models](http://arxiv.org/abs/2505.10554v2)** | 2025-05-27 | In Progress |
| **[Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO](http://arxiv.org/abs/2505.21457v1)** | 2025-05-27 | <details><summary>Proje...</summary><p>Project Page: https://aim-uofa.github.io/ACTIVE-o3</p></details> |
| **[One-shot Entropy Minimization](http://arxiv.org/abs/2505.20282v2)** | 2025-05-27 | Work in progress |
| **[Can Large Reasoning Models Self-Train?](http://arxiv.org/abs/2505.21444v1)** | 2025-05-27 | <details><summary>Proje...</summary><p>Project website: https://self-rewarding-llm-training.github.io/</p></details> |
| **[A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment](http://arxiv.org/abs/2505.21414v1)** | 2025-05-27 |  |
| **[MRSD: Multi-Resolution Skill Discovery for HRL Agents](http://arxiv.org/abs/2505.21410v1)** | 2025-05-27 |  |
| **[Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback](http://arxiv.org/abs/2411.01834v2)** | 2025-05-27 | Accepted by ACL 2025 |
| **[Finite Sample Analysis of Linear Temporal Difference Learning with Arbitrary Features](http://arxiv.org/abs/2505.21391v1)** | 2025-05-27 |  |
| **[Linear $Q$-Learning Does Not Diverge in $L^2$: Convergence Rates to a Bounded Set](http://arxiv.org/abs/2501.19254v4)** | 2025-05-27 |  |
| **[Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning](http://arxiv.org/abs/2505.17266v2)** | 2025-05-27 |  |
| **[DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement Learning Agents](http://arxiv.org/abs/2502.01956v2)** | 2025-05-27 |  |
| **[Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation](http://arxiv.org/abs/2405.00715v6)** | 2025-05-27 |  |
| **[Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling](http://arxiv.org/abs/2505.17659v2)** | 2025-05-27 |  |

## Robotics
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[CLAMP: Crowdsourcing a LArge-scale in-the-wild haptic dataset with an open-source device for Multimodal robot Perception](http://arxiv.org/abs/2505.21495v1)** | 2025-05-27 |  |
| **[EquAct: An SE(3)-Equivariant Multi-Task Transformer for Open-Loop Robotic Manipulation](http://arxiv.org/abs/2505.21351v1)** | 2025-05-27 |  |
| **[EgoWalk: A Multimodal Dataset for Robot Navigation in the Wild](http://arxiv.org/abs/2505.21282v1)** | 2025-05-27 |  |
| **[CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration](http://arxiv.org/abs/2403.18459v2)** | 2025-05-27 | <details><summary>7 pag...</summary><p>7 pages, 8 figures, accepted at IEEE IROS 2024</p></details> |
| **[Human-Centered Development of Guide Dog Robots: Quiet and Stable Locomotion Control](http://arxiv.org/abs/2505.11808v2)** | 2025-05-27 |  |
| **[Efficient Robotic Policy Learning via Latent Space Backward Planning](http://arxiv.org/abs/2505.06861v2)** | 2025-05-27 | <details><summary>Accep...</summary><p>Accepted by ICML 2025</p></details> |
| **[Object-Centric Action-Enhanced Representations for Robot Visuo-Motor Policy Learning](http://arxiv.org/abs/2505.20962v1)** | 2025-05-27 |  |
| **[Spatial RoboGrasp: Generalized Robotic Grasping Control Policy](http://arxiv.org/abs/2505.20814v1)** | 2025-05-27 |  |
| **[Learning Generalizable Robot Policy with Human Demonstration Video as a Prompt](http://arxiv.org/abs/2505.20795v1)** | 2025-05-27 |  |
| **[From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation](http://arxiv.org/abs/2505.08548v2)** | 2025-05-27 | <details><summary>Our p...</summary><p>Our project homepage: https://embodied-fsd.github.io/</p></details> |
| **[JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes](http://arxiv.org/abs/2505.06771v2)** | 2025-05-27 | <details><summary>22 pa...</summary><p>22 pages, 14 figures, 10 tables. https://github.com/GT-STAR-Lab/JaxRobotarium</p></details> |
| **[RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation](http://arxiv.org/abs/2412.13877v3)** | 2025-05-27 | <details><summary>21 pa...</summary><p>21 pages, 17 figures, Robotics: Science and Systems 2025</p></details> |
| **[Collision- and Reachability-Aware Multi-Robot Control with Grounded LLM Planners](http://arxiv.org/abs/2505.20573v1)** | 2025-05-26 |  |
| **[Developing a Robotic Surgery Training System for Wide Accessibility and Research](http://arxiv.org/abs/2505.20562v1)** | 2025-05-26 | <details><summary>6 pag...</summary><p>6 pages, The IEEE International Conference on Advanced Robotics and Mechatronics (IEEE ARM 2025), accepted</p></details> |
| **[CoRI: Synthesizing Communication of Robot Intent for Physical Human-Robot Interaction](http://arxiv.org/abs/2505.20537v1)** | 2025-05-26 | 33 pages, 10 figures |

